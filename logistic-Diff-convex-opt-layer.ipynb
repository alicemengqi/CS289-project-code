{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import torch \n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min log(1+exp(-z*w(x)))\n",
    "#   policy w(x)  \n",
    "#    z \\in  {-1, 1}\n",
    "# w(x) \\in [-10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this specifies the training process\n",
    "def train(iters, Xtrain, ztrain):\n",
    "    p = Xtrain.shape[1]\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "          super(Net, self).__init__()\n",
    "          # First fully connected layer\n",
    "          self.fc1 = nn.Linear(p, 32)\n",
    "          # Second fully connected layer that outputs \n",
    "          self.fc2 = nn.Linear(32, 1)\n",
    "        def forward(self, x):\n",
    "            x = nn.LeakyReLU(0.1)(self.fc1(x))\n",
    "            x = nn.LeakyReLU(0.1)(self.fc2(x))\n",
    "            return x\n",
    "\n",
    "\n",
    "    dim0 = ztrain.shape[0]\n",
    "    ztrain = ztrain.reshape((dim0,1))\n",
    "\n",
    "    w_cvxpy = cp.Variable(1)\n",
    "    z_cvxpy = cp.Parameter(1)\n",
    "    objective = cp.logistic(1+ cp.exp(-cp.multiply(z_cvxpy,w_cvxpy))) + 0.001*cp.norm(w_cvxpy)**2 \n",
    "    problem = cp.Problem(cp.Minimize(objective), [w_cvxpy <= 10, w_cvxpy >= -10 ])\n",
    "    assert problem.is_dpp()\n",
    "    net = Net()\n",
    "    cvxlayer = CvxpyLayer(problem, [z_cvxpy], [w_cvxpy])\n",
    "    results = []\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01)  \n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    for i in range(iters):\n",
    "        out1 = net(Xtrain) # first build a regular NN\n",
    "#         print(out1)\n",
    "        out, = cvxlayer(out1) # attach an additional cvxpy layer\n",
    "#         print(out)\n",
    "        loss = torch.mean(torch.log(torch.add(torch.exp(torch.neg(torch.multiply(out,ztrain))), 1)))    \n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        loss.backward()\n",
    "#         for name, param in net.named_parameters():\n",
    "#             if param.requires_grad:\n",
    "#                 print (name, param.data)\n",
    "        optimizer.step()    # Does the update\n",
    "        results.append(loss.item())\n",
    "        print(\"(iter %d) loss: %g \" % (i, results[-1]))\n",
    "    return results, net, cvxlayer\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, cvxlayer, Xtest, ztest):\n",
    "    z_pred = net(Xtest)\n",
    "    w_test, = cvxlayer(z_pred)\n",
    "    return w_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_cvxlayer(Xtrain, ztrain, Xtest, ztest):\n",
    "    Xtrain = Xtrain.astype(np.float32)\n",
    "    ztrain = ztrain.astype(np.float32)\n",
    "    Xtest = Xtest.astype(np.float32)\n",
    "    ztest = ztest.astype(np.float32)\n",
    "    print(ztest.shape)\n",
    "    Xtrain, Xtest, ztrain, ztest = map(\n",
    "        torch.from_numpy, [Xtrain, Xtest, ztrain, ztest])\n",
    "    print(ztest.shape)\n",
    "    results, net, cvxlayer = train(100, Xtrain, ztrain)\n",
    "    out = predict(net, cvxlayer, Xtest, ztest)\n",
    "    print(out.shape)\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_csv = np.genfromtxt('x_train.csv' , delimiter=\",\")\n",
    "\n",
    "Xtrain = Xtrain_csv[1:,:]\n",
    "\n",
    "ztrain_csv = np.genfromtxt('z_train.csv' , delimiter=\",\")\n",
    "\n",
    "ztrain = ztrain_csv[1:]\n",
    "\n",
    "Xtest_csv = np.genfromtxt('x_test.csv' , delimiter=\",\")\n",
    "\n",
    "Xtest = Xtest_csv[1:,:]\n",
    "\n",
    "ztest_csv = np.genfromtxt('z_test.csv' , delimiter=\",\")\n",
    "\n",
    "ztest = ztest_csv[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n",
      "torch.Size([4000])\n",
      "(iter 0) loss: 1.26213 \n",
      "(iter 1) loss: 0.817434 \n",
      "(iter 2) loss: 0.772052 \n",
      "(iter 3) loss: 0.737027 \n",
      "(iter 4) loss: 0.708787 \n",
      "(iter 5) loss: 0.685319 \n",
      "(iter 6) loss: 0.665365 \n",
      "(iter 7) loss: 0.648105 \n",
      "(iter 8) loss: 0.632965 \n",
      "(iter 9) loss: 0.619533 \n",
      "(iter 10) loss: 0.607506 \n",
      "(iter 11) loss: 0.596651 \n",
      "(iter 12) loss: 0.586788 \n",
      "(iter 13) loss: 0.577774 \n",
      "(iter 14) loss: 0.569495 \n",
      "(iter 15) loss: 0.561854 \n",
      "(iter 16) loss: 0.554774 \n",
      "(iter 17) loss: 0.548188 \n",
      "(iter 18) loss: 0.542045 \n",
      "(iter 19) loss: 0.536298 \n",
      "(iter 20) loss: 0.530905 \n",
      "(iter 21) loss: 0.525835 \n",
      "(iter 22) loss: 0.521056 \n",
      "(iter 23) loss: 0.516542 \n",
      "(iter 24) loss: 0.51227 \n",
      "(iter 25) loss: 0.508221 \n",
      "(iter 26) loss: 0.504377 \n",
      "(iter 27) loss: 0.500722 \n",
      "(iter 28) loss: 0.497239 \n",
      "(iter 29) loss: 0.493918 \n",
      "(iter 30) loss: 0.490748 \n",
      "(iter 31) loss: 0.487717 \n",
      "(iter 32) loss: 0.484815 \n",
      "(iter 33) loss: 0.482037 \n",
      "(iter 34) loss: 0.479374 \n",
      "(iter 35) loss: 0.476814 \n",
      "(iter 36) loss: 0.474354 \n",
      "(iter 37) loss: 0.471989 \n",
      "(iter 38) loss: 0.469716 \n",
      "(iter 39) loss: 0.467524 \n",
      "(iter 40) loss: 0.465409 \n",
      "(iter 41) loss: 0.463372 \n",
      "(iter 42) loss: 0.461406 \n",
      "(iter 43) loss: 0.459506 \n",
      "(iter 44) loss: 0.457669 \n",
      "(iter 45) loss: 0.455896 \n",
      "(iter 46) loss: 0.454178 \n",
      "(iter 47) loss: 0.452514 \n",
      "(iter 48) loss: 0.450903 \n",
      "(iter 49) loss: 0.449342 \n",
      "(iter 50) loss: 0.447827 \n",
      "(iter 51) loss: 0.446358 \n",
      "(iter 52) loss: 0.444931 \n",
      "(iter 53) loss: 0.443547 \n",
      "(iter 54) loss: 0.442203 \n",
      "(iter 55) loss: 0.440895 \n",
      "(iter 56) loss: 0.439625 \n",
      "(iter 57) loss: 0.438389 \n",
      "(iter 58) loss: 0.437188 \n",
      "(iter 59) loss: 0.436018 \n",
      "(iter 60) loss: 0.434878 \n",
      "(iter 61) loss: 0.433769 \n",
      "(iter 62) loss: 0.432689 \n",
      "(iter 63) loss: 0.431635 \n",
      "(iter 64) loss: 0.430609 \n",
      "(iter 65) loss: 0.429608 \n",
      "(iter 66) loss: 0.428631 \n",
      "(iter 67) loss: 0.427679 \n",
      "(iter 68) loss: 0.426749 \n",
      "(iter 69) loss: 0.425841 \n",
      "(iter 70) loss: 0.424955 \n",
      "(iter 71) loss: 0.424088 \n",
      "(iter 72) loss: 0.423243 \n",
      "(iter 73) loss: 0.422415 \n",
      "(iter 74) loss: 0.421608 \n",
      "(iter 75) loss: 0.420817 \n",
      "(iter 76) loss: 0.420044 \n",
      "(iter 77) loss: 0.419289 \n",
      "(iter 78) loss: 0.41855 \n",
      "(iter 79) loss: 0.417827 \n",
      "(iter 80) loss: 0.417119 \n",
      "(iter 81) loss: 0.416426 \n",
      "(iter 82) loss: 0.415747 \n",
      "(iter 83) loss: 0.415081 \n",
      "(iter 84) loss: 0.414431 \n",
      "(iter 85) loss: 0.413793 \n",
      "(iter 86) loss: 0.413168 \n",
      "(iter 87) loss: 0.412556 \n",
      "(iter 88) loss: 0.411955 \n",
      "(iter 89) loss: 0.411368 \n",
      "(iter 90) loss: 0.41079 \n",
      "(iter 91) loss: 0.410225 \n",
      "(iter 92) loss: 0.409669 \n",
      "(iter 93) loss: 0.409125 \n",
      "(iter 94) loss: 0.408592 \n",
      "(iter 95) loss: 0.408067 \n",
      "(iter 96) loss: 0.407552 \n",
      "(iter 97) loss: 0.407047 \n",
      "(iter 98) loss: 0.406552 \n",
      "(iter 99) loss: 0.406066 \n",
      "torch.Size([4000, 1])\n"
     ]
    }
   ],
   "source": [
    "w_test = main_cvxlayer(Xtrain, ztrain, Xtest, ztest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8176],\n",
       "        [2.6402],\n",
       "        [2.8070],\n",
       "        ...,\n",
       "        [3.0660],\n",
       "        [3.1190],\n",
       "        [2.4703]], grad_fn=<_CvxpyLayerFnFnBackward>)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_nn = w_test.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"w_test.csv\", w_nn, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
